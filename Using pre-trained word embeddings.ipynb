{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c049c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31db8638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\n",
      "17329808/17329808 [==============================] - 40s 2us/step\n"
     ]
    }
   ],
   "source": [
    "data_path = keras.utils.get_file(\n",
    "    \"news20.tar.gz\",\n",
    "    \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\",\n",
    "    untar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b14daa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of directories: 20\n",
      "Directory names: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "Number of files in comp.graphics: 1000\n",
      "Some example filenames: ['37261', '37913', '37914', '37915', '37916']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "data_dir = pathlib.Path(data_path).parent / \"20_newsgroup\"\n",
    "dirnames = os.listdir(data_dir)\n",
    "print(\"Number of directories:\", len(dirnames))\n",
    "print(\"Directory names:\", dirnames)\n",
    "\n",
    "fnames = os.listdir(data_dir / \"comp.graphics\")\n",
    "print(\"Number of files in comp.graphics:\", len(fnames))\n",
    "print(\"Some example filenames:\", fnames[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5de4f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newsgroups: comp.graphics\n",
      "Path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!howland.reston.ans.net!agate!dog.ee.lbl.gov!network.ucsd.edu!usc!rpi!nason110.its.rpi.edu!mabusj\n",
      "From: mabusj@nason110.its.rpi.edu (Jasen M. Mabus)\n",
      "Subject: Looking for Brain in CAD\n",
      "Message-ID: <c285m+p@rpi.edu>\n",
      "Nntp-Posting-Host: nason110.its.rpi.edu\n",
      "Reply-To: mabusj@rpi.edu\n",
      "Organization: Rensselaer Polytechnic Institute, Troy, NY.\n",
      "Date: Thu, 29 Apr 1993 23:27:20 GMT\n",
      "Lines: 7\n",
      "\n",
      "Jasen Mabus\n",
      "RPI student\n",
      "\n",
      "\tI am looking for a hman brain in any CAD (.dxf,.cad,.iges,.cgm,etc.) or picture (.gif,.jpg,.ras,etc.) format for an animation demonstration. If any has or knows of a location please reply by e-mail to mabusj@rpi.edu.\n",
      "\n",
      "Thank you in advance,\n",
      "Jasen Mabus  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(open(data_dir / \"comp.graphics\" / \"38987\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3537723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing alt.atheism, 1000 files found\n",
      "Processing comp.graphics, 1000 files found\n",
      "Processing comp.os.ms-windows.misc, 1000 files found\n",
      "Processing comp.sys.ibm.pc.hardware, 1000 files found\n",
      "Processing comp.sys.mac.hardware, 1000 files found\n",
      "Processing comp.windows.x, 1000 files found\n",
      "Processing misc.forsale, 1000 files found\n",
      "Processing rec.autos, 1000 files found\n",
      "Processing rec.motorcycles, 1000 files found\n",
      "Processing rec.sport.baseball, 1000 files found\n",
      "Processing rec.sport.hockey, 1000 files found\n",
      "Processing sci.crypt, 1000 files found\n",
      "Processing sci.electronics, 1000 files found\n",
      "Processing sci.med, 1000 files found\n",
      "Processing sci.space, 1000 files found\n",
      "Processing soc.religion.christian, 997 files found\n",
      "Processing talk.politics.guns, 1000 files found\n",
      "Processing talk.politics.mideast, 1000 files found\n",
      "Processing talk.politics.misc, 1000 files found\n",
      "Processing talk.religion.misc, 1000 files found\n",
      "Classes: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "Number of samples: 19997\n"
     ]
    }
   ],
   "source": [
    "'''From the privous visualisation we recognize that there are header lines that are leaking the file's category. so we need to\n",
    "handle them'''\n",
    "samples = []\n",
    "labels = []\n",
    "class_names = []\n",
    "class_index = 0\n",
    "for dirname in sorted(os.listdir(data_dir)):\n",
    "    class_names.append(dirname)\n",
    "    dirpath = data_dir / dirname\n",
    "    fnames = os.listdir(dirpath)\n",
    "'''priting the number of files in this category'''\n",
    "    print(\"Processing %s, %d files found\" % (dirname, len(fnames)))\n",
    "'''Removing spaces (\\n) and headers ( 10 first lines)'''\n",
    "    for fname in fnames:\n",
    "        fpath = dirpath / fname\n",
    "        f = open(fpath, encoding=\"latin-1\")\n",
    "        content = f.read()\n",
    "        lines = content.split(\"\\n\")\n",
    "        lines = lines[10:]\n",
    "        content = \"\\n\".join(lines)\n",
    "        samples.append(content)\n",
    "        labels.append(class_index)\n",
    "    class_index += 1\n",
    "\n",
    "print(\"Classes:\", class_names)\n",
    "print(\"Number of samples:\", len(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef23d78",
   "metadata": {},
   "source": [
    "## Shuffle and split the data into training & validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1277d9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''In this step we set and split it into two parts ( Train and test sets ).\n",
    "The way we will split is :  80:20 ratio (train:validation)\n",
    "'''\n",
    "#  Becau\n",
    "# Shuffle the data\n",
    "seed = 1337\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(samples)\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(labels)\n",
    "\n",
    "# Extract a training & validation split\n",
    "validation_split = 0.2\n",
    "num_validation_samples = int(validation_split * len(samples))\n",
    "train_samples = samples[:-num_validation_samples]\n",
    "val_samples = samples[-num_validation_samples:]\n",
    "train_labels = labels[:-num_validation_samples]\n",
    "val_labels = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61f768a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# To normalize, split, and map strings to integers we need to add a vectorization layer for texts\n",
    "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "# Now we will create a tensor flow dataset from the output vectorized\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5df6144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'to', 'of']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the top 5 words\n",
    "vectorizer.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5d678b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2, 3456, 1682,   15,    2, 5776], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will vectorize a text and visualize the output\n",
    "output = vectorizer([[\"the cat sat on the mat\"]])\n",
    "output.numpy()[0, :6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee2be9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dict mapping words to their indices:\n",
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa7b705c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3456, 1682, 15, 2, 5776]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will try to vectorize the same text and we will obtain the same result\n",
    "test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "[word_index[w] for w in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eec1ad0",
   "metadata": {},
   "source": [
    "## Load pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "226deef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "'''The Dataset we downloaded contains text-encoded vectors of various sizes, but we will use the 100d ones only'''\n",
    "#First we will import the text file\n",
    "path_to_glove_file = os.path.join(\n",
    "    os.path.expanduser(\"~\"), \".keras\\\\datasets\\\\glove.6B.100d.txt\"\n",
    ")\n",
    "\n",
    "embeddings_index = {}\n",
    "# Here we will count number of word vectors, we need to specify the UTF-8 encoder\n",
    "with open(path_to_glove_file, encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "792fb164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 18019 words (1981 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "''' we will prepare an embedding matrix'''\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "'''The entry at index i is the pre-trained vector for the word of index i in our vectorizer's vocabulary'''\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8730eafc",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e2ed93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "'''Before building the model we will the embedding layer and set trainable to False to keep the embeddings fixed'''\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ad44736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         2000200   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 128)         64128     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, None, 128)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, None, 128)         82048     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, None, 128)        0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, None, 128)         82048     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                2580      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,247,516\n",
      "Trainable params: 247,316\n",
      "Non-trainable params: 2,000,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "'''\n",
    "Now we will build a simple 1D convnet starting with an Embedding layer.\n",
    "Before fitting our model, we choosed ReLU as loss function from hiddel layer, and softmax activation function for output layer.\n",
    "'''\n",
    "\n",
    "# A integer input for vocab indices.\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "\n",
    "# Conv1D + global max pooling\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# We add a hidden layer:\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a softmax:\n",
    "preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "model = keras.Model(int_sequences_input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a579ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using the train and test datasets.\n",
    "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
    "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88feb893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "125/125 [==============================] - 18s 98ms/step - loss: 2.6658 - acc: 0.1392 - val_loss: 2.1176 - val_acc: 0.2598\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 13s 106ms/step - loss: 1.9241 - acc: 0.3368 - val_loss: 1.7784 - val_acc: 0.3936\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 13s 106ms/step - loss: 1.5059 - acc: 0.4826 - val_loss: 1.3214 - val_acc: 0.5371\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 16s 124ms/step - loss: 1.2722 - acc: 0.5628 - val_loss: 1.1670 - val_acc: 0.6004\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 18s 140ms/step - loss: 1.1028 - acc: 0.6239 - val_loss: 1.0947 - val_acc: 0.6304\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 16s 129ms/step - loss: 0.9736 - acc: 0.6660 - val_loss: 1.0277 - val_acc: 0.6492\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 15s 117ms/step - loss: 0.8484 - acc: 0.7015 - val_loss: 1.0208 - val_acc: 0.6592\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 15s 124ms/step - loss: 0.7630 - acc: 0.7292 - val_loss: 1.0763 - val_acc: 0.6524\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 14s 116ms/step - loss: 0.6785 - acc: 0.7646 - val_loss: 0.9260 - val_acc: 0.6952\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 14s 116ms/step - loss: 0.5942 - acc: 0.7885 - val_loss: 0.9610 - val_acc: 0.6902\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 12s 98ms/step - loss: 0.5164 - acc: 0.8145 - val_loss: 1.0210 - val_acc: 0.6892\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 12s 97ms/step - loss: 0.4666 - acc: 0.8375 - val_loss: 1.0441 - val_acc: 0.6809\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 0.3906 - acc: 0.8631 - val_loss: 1.1584 - val_acc: 0.6792\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 12s 98ms/step - loss: 0.3601 - acc: 0.8742 - val_loss: 1.3885 - val_acc: 0.6592\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 12s 97ms/step - loss: 0.3142 - acc: 0.8894 - val_loss: 1.2380 - val_acc: 0.6964\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 12s 97ms/step - loss: 0.2878 - acc: 0.9028 - val_loss: 1.1817 - val_acc: 0.7164\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 12s 97ms/step - loss: 0.2464 - acc: 0.9126 - val_loss: 1.3414 - val_acc: 0.6907\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 14s 115ms/step - loss: 0.2404 - acc: 0.9191 - val_loss: 1.3197 - val_acc: 0.7099\n",
      "Epoch 19/20\n",
      "125/125 [==============================] - 15s 123ms/step - loss: 0.2035 - acc: 0.9309 - val_loss: 1.2473 - val_acc: 0.7159\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 15s 121ms/step - loss: 0.1962 - acc: 0.9345 - val_loss: 1.3270 - val_acc: 0.7204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x169e02da830>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# because we're doing softmax classification we will use categorical crossentropy as our loss\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90e83ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 577ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'comp.graphics'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
    "x = vectorizer(string_input)\n",
    "preds = model(x)\n",
    "end_to_end_model = keras.Model(string_input, preds)\n",
    "\n",
    "probabilities = end_to_end_model.predict(\n",
    "    [[\"this message is about computer graphics and 3D modeling\"]]\n",
    ")\n",
    "\n",
    "class_names[np.argmax(probabilities[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90823798",
   "metadata": {},
   "source": [
    "In this example we have build and trained a text classification model with the Newsgroup20 dataset. We create a vectorization layer for word splitting & indexing. Because our model is using pre-trained word embeddings we prepared an embedding matrix that we can use in a Keras Embedding layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
