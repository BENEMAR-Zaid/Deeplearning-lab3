{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2de89d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "031f5c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Number of batches in raw_train_ds: 625\n",
      "Number of batches in raw_val_ds: 157\n",
      "Number of batches in raw_test_ds: 782\n"
     ]
    }
   ],
   "source": [
    "'''In this step we download and load the data set and split it into two parts ( Train and test sets ) then printing the shape \n",
    "of each set. The way we will split is :  90:5:5 ratio (train:validation:test)\n",
    "'''\n",
    "#  Because we have a set of text files we will use tf.keras.preprocessing.text_dataset_from_directory to generate a labeled tf.data.Dataset\n",
    "batch_size = 32\n",
    "#Here we are generating train set from train directory, the size is 80%\n",
    "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    ")\n",
    "#Here we are generating validation set from train directory, the size is 20%\n",
    "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    ")\n",
    "#Here we are generating test set from test directory\n",
    "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")\n",
    "#Finally we will visualize the number of exemples in each created set\n",
    "print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2db2c2bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'I am very disappointed with \"K-911.\" The original \"good\" quality of \"K-9\" doesn\\'t exist any more. This is more like a sitcom! Some of casts from original movie returned and got some of my memory back. The captain of Dooley now loves to hit him like a scene from old comedy show. That was crazy. What\\'s the deal with the change of Police? It seems like they are now LAPD! Not San Diego PD. It is a completely different movie from \"'\n",
      "0\n",
      "b\"Giallo fans, seek out this rare film. It is well written, and full of all sorts of the usual low lifes that populate these films. I don't want to give anything away, so I wont even say anything about the plot. The whole movie creates a very bizarre atmosphere, and you don't know what to expect or who to suspect. Recommended! The only place I've seen to get this film in english is from European Trash Cinema, for $15.\"\n",
      "1\n",
      "b\"Terry Gilliam's and David Peoples' teamed up to create one of the most intelligent and creative science fiction movies of the '90's. People's proved a screenplay with bizarre twists and fantastic ideas about the nature of time \\xc2\\x97 I especially love the idea one can't change the past; it's a nice counterpoint to so many time-travelling movies which say otherwise \\xc2\\x97 biological holocausts and the thin line between sanity and madness. Gilliam visualized his ideas with unique quirkiness, perfection and originality.<br /><br />The story itself is engaging: one man, James Cole (played by Bruce Willis in a heart-warming performance) travels several decades to the past to retrieve information about a virus that's wiped out mankind and left only a few survivors alive living underground: with the information he'll collect, scientists hope to find a cure so everyone in the future can return to the surface. But because their time-travelling technology isn't perfect, he ends up being sent towards different other pasts and complicating things. And from that a brilliant science fiction thriller with shades of film noir ensues as the multiple pieces of a huge jigsaw start fitting together to form a bizarre narrative involving animal right activists, end of the millennium paranoia, biological weapons, the perception of reality, and the definition of sanity. With such a complex movie, it was easy for Gilliam and Peoples to create a mess, but instead Twelve Monkeys is a thought-provoking narrative which will please those who like to be challenged and have patience to appreciate some crazy ideas.<br /><br />I watched this movie once around 10 years ago. It marked me a lot: I remember still thinking about many days after-wards; for my young mind this seemed quite mind-blowing and it was one of the first movies to make me appreciate cinema as something serious and important. I've re-watched this movie a few days ago on DVD and it's better than I remembered it. Brad Pitt still steals all the scenes he's in, playing Jeffrey Goines \\xc2\\x97 almost a prelude to his Tyler Durden character in Fight Club \\xc2\\x97 a rich kid with some anarchist/non-conformist ideas who's also crazy and, according to Cole, perhaps responsible for the virus. The scenes between Jeffrey and Cole in the madhouse are the best in the movie, Pitt's eyes, voice and quirky mannerisms convince you he's really a crazy guy locked in a warped logic only he understands. Pitt's Oscar nomination was well deserved! Surprising was also Bruce Willis' performance: his I didn't remember very well, but it's beautiful and full of sensibility; he plays a man who spent almost all his life underground, and when he comes to the past you'll share his childish fascination with something as simple as breathing the fresh air of the morning or watching the sun go up. Cole is a rather ambiguous character, Peoples' tried to imbue some darkness in him, and he does other disturbing things to other people and to himself: the scene where he removes his own teeth reveals how far his dementia has gone unchecked. Ironically Cole didn't start as a crazy character, but when he starts warning everyone about the end of the world, he's considered mad and convinced it's all in his mind, until he arrives at a point when he can't distinguish past from future, reality from fiction. Willis spends a lot of time looking confused and insecure, and it works perfectly. One of the fun twists in the narrative is when Cole's shrink, Dr. Kathryn Railly, finds undeniable proof he's really from the future and now has to convince him again of his mission to save the world. The screenplay is full with weird twists like this and it keeps the movie in a fast pace. Their relationship is also well-handed, although perhaps a bit compressed for time's sake. But I enjoyed watching Cole and Railly falling in love and trying to escape the authority of the future to live a peaceful life in the past. But then things end in a tragic/bittersweet climax at an airport, wrapping all the pieces together, which will blow many minds away.<br /><br />There are two great endings in this movie, a twist in the sense of Se7en or Fight Club, and a more intimate ending where Railly is crouching next to Cole who's just been shot and looking around for a younger James Cole who's witnessing his future self die; the two share a brief look, and she smiles at him. The twist is brilliant, but I prefer this ending for emotional impact. Madeleine Stowe is very good playing Dr. Railly, she drew many different emotions from me in her performance. The movie is filled with a sense of fatalism with the idea the past can't be changed: this movie shows that in a terrifying way. It reminds me of Chinatown in that sense, the way Jake Gittes messes everything up the more he tries to help. Railly's character shares that fatalism, the more she tries to help Cole \\xc2\\x97 first dealing with his 'madness' then helping him in his mission \\xc2\\x97 the more they're sucked into tragedy.<br /><br />The twist ends with a hopeful note, though, with the feeling Cole's mission hasn't been in vain. Twelve Monkeys is a great movie to watch if one wants to be entertained; it's not supposed to be art, although it's more artists than many artistic movies. It's an unpretentious movie where all elements, from music to editing to costume design, etc., came together beautifully to produce a modern cinema masterpiece.\"\n",
      "1\n",
      "b\"We expected something great when we went to see this bomb. It is basically a Broadway play put on film. The music is plain terrible. There isn't one memorable song in the movie -- heard any hits from this movie? You won't because there aren't any. Some of the musical numbers go on so long that I got up to go to the restroom and get some pop corn and it was still going when I got back! If they were good songs well -- but they suck. The pace is slow, terrible character development. The lead was praised for her singing but sounded like she screamed every song -- it was almost impossible to stand. This movie has NOTHING to offer anyone but die-hard Broadway enthusiasts. This is without a doubt the most over rated movie I've seen in my entire life. A complete waist of time and money. There is nothing memorable about this movie except Danny Glover -- who wasn't on screen enough and whose character wasn't developed enough. Rent the video and you'll agree -- this movie was an expensive, over produced, polished dog do.\"\n",
      "0\n",
      "b'I understand this movie was made on a very low budget but that is no excuse for the monstrosity that is Grendel. Deathstalker, The Throne of Fire, Barbarian Queen, Conquest, the Invincible Barbarian were all done on shoestring budgets and poor special effects yet they still managed to create cult classics by adding some scantily clad women warriors and a good sense of humor. The primitive costumes, dark castles and beautiful Bulgarian landscape gave Grendel the potential to be a very good low budget sword and sorcery film, but the makers completely ruined this opportunity by using extremely poor CGI effects and colorless characters. Compare this film to Beowulf (1999). It may not be Citizen Kane but it is a good example of how an entertaining low budget sci-fi/ adventure movie can be made by using credible special effects and appealing characters.'\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# It's important to take a look at your raw data to ensure your normalization\n",
    "# and tokenization will work as expected. We can do that by taking a few\n",
    "# examples from the training set and looking at them.\n",
    "# This is one of the places where eager execution shines:\n",
    "# we can just evaluate these tensors using .numpy()\n",
    "# instead of needing to evaluate them in a Session/Graph context.\n",
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "    for i in range(5):\n",
    "        print(text_batch.numpy()[i])\n",
    "        print(label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5e10283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "import string\n",
    "import re\n",
    "\n",
    "#We will remove the HTML break tags \"<br /> from the text\", and we will create a custom standardization function\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Model constants.\n",
    "max_features = 20000\n",
    "embedding_dim = 128\n",
    "sequence_length = 500\n",
    "\n",
    "# To normalize, split, and map strings to integers we need to add a vectorization layer for texts\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "\n",
    "# Let's make a text-only dataset (no labels):\n",
    "text_ds = raw_train_ds.map(lambda x, y: x)\n",
    "# Let's call `adapt`:\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9864c4",
   "metadata": {},
   "source": [
    "# Vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a47a5c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "\n",
    "# Vectorize the data.\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)\n",
    "\n",
    "# Do async prefetching / buffering of the data for best performance on GPU.\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effce67d",
   "metadata": {},
   "source": [
    "# Build model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f06aa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "'''\n",
    "Now we will build a simple 1D convnet starting with an Embedding layer.\n",
    "Before fitting our model, we choosed ReLU as loss function from hiddel layer, and sigmoid activation function for output layer.\n",
    "'''\n",
    "# A integer input for vocab indices.\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "# 'embedding_dim'.\n",
    "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Conv1D + global max pooling\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# We add a hidden layer:\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, predictions)\n",
    "\n",
    "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749f0f7a",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49d436dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "625/625 [==============================] - 71s 109ms/step - loss: 0.4820 - accuracy: 0.7358 - val_loss: 0.3154 - val_accuracy: 0.8656\n",
      "Epoch 2/3\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.2160 - accuracy: 0.9161 - val_loss: 0.3182 - val_accuracy: 0.8762\n",
      "Epoch 3/3\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.1146 - accuracy: 0.9579 - val_loss: 0.3888 - val_accuracy: 0.8682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2151ab4a260>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "# Fit the model using the train and test datasets.\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ecfc49",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35deaa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 137s 172ms/step - loss: 0.3998 - accuracy: 0.8601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3997553884983063, 0.8600800037384033]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3747cbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 14s 16ms/step - loss: 0.3998 - accuracy: 0.8601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3997551202774048, 0.8600800037384033]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''To create model that can processe raw strings, we can use the weight we trained'''\n",
    "# A string input\n",
    "inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
    "# Turn strings into vocab indices\n",
    "indices = vectorize_layer(inputs)\n",
    "# Turn vocab indices into predictions\n",
    "outputs = model(indices)\n",
    "\n",
    "# Our end to end model\n",
    "end_to_end_model = tf.keras.Model(inputs, outputs)\n",
    "end_to_end_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Test it with `raw_test_ds`, which yields raw strings\n",
    "end_to_end_model.evaluate(raw_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fc2761",
   "metadata": {},
   "source": [
    "In this example we have build and trained a text classification model starting from raw text. We remove the HTML space tags andc create our standardization function, and We create a vectorization layer for word splitting & indexing. Finally we build a model using the weight we trained that capable of preprocessing raw strings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
