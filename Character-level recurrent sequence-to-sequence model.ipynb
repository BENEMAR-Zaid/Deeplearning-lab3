{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ad0a5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d93f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' # Setting the hyper parameters of our model '''\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = \"fra.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "957fd0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 71\n",
      "Number of unique output tokens: 93\n",
      "Max sequence length for inputs: 15\n",
      "Max sequence length for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "# Read the file data and specifie delimiter\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "# For each line in the file we will extract input text and target text, they are delimited by a tab\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split(\"\\t\")\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = \"\\t\" + target_text + \"\\n\"\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "# we will add each character in the input/ ouput character without repetition\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "# No we will visualise the number of exemples we have\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print(\"Number of samples:\", len(input_texts))\n",
    "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
    "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
    "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
    "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153020d9",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c18d95b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
    "encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90a8091e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "125/125 [==============================] - 35s 232ms/step - loss: 1.1405 - accuracy: 0.7366 - val_loss: 1.0103 - val_accuracy: 0.7205\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - 24s 194ms/step - loss: 0.8147 - accuracy: 0.7794 - val_loss: 0.8392 - val_accuracy: 0.7697\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - 24s 189ms/step - loss: 0.6610 - accuracy: 0.8162 - val_loss: 0.6955 - val_accuracy: 0.8013\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - 24s 194ms/step - loss: 0.5711 - accuracy: 0.8342 - val_loss: 0.6353 - val_accuracy: 0.8144\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 0.5217 - accuracy: 0.8474 - val_loss: 0.5817 - val_accuracy: 0.8308\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 0.4858 - accuracy: 0.8571 - val_loss: 0.5495 - val_accuracy: 0.8407\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 0.4587 - accuracy: 0.8638 - val_loss: 0.5306 - val_accuracy: 0.8441\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - 24s 193ms/step - loss: 0.4359 - accuracy: 0.8703 - val_loss: 0.5125 - val_accuracy: 0.8490\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - 24s 194ms/step - loss: 0.4164 - accuracy: 0.8757 - val_loss: 0.5006 - val_accuracy: 0.8528\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - 24s 192ms/step - loss: 0.3989 - accuracy: 0.8801 - val_loss: 0.4871 - val_accuracy: 0.8570\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - 24s 189ms/step - loss: 0.3826 - accuracy: 0.8850 - val_loss: 0.4814 - val_accuracy: 0.8575\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - 24s 194ms/step - loss: 0.3674 - accuracy: 0.8890 - val_loss: 0.4699 - val_accuracy: 0.8611\n",
      "Epoch 13/100\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.3526 - accuracy: 0.8936 - val_loss: 0.4592 - val_accuracy: 0.8639\n",
      "Epoch 14/100\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.3393 - accuracy: 0.8973 - val_loss: 0.4546 - val_accuracy: 0.8664\n",
      "Epoch 15/100\n",
      "125/125 [==============================] - 25s 196ms/step - loss: 0.3267 - accuracy: 0.9012 - val_loss: 0.4497 - val_accuracy: 0.8678\n",
      "Epoch 16/100\n",
      "125/125 [==============================] - 24s 194ms/step - loss: 0.3149 - accuracy: 0.9050 - val_loss: 0.4458 - val_accuracy: 0.8693\n",
      "Epoch 17/100\n",
      "125/125 [==============================] - 25s 196ms/step - loss: 0.3033 - accuracy: 0.9082 - val_loss: 0.4426 - val_accuracy: 0.8719\n",
      "Epoch 18/100\n",
      "125/125 [==============================] - 24s 193ms/step - loss: 0.2927 - accuracy: 0.9114 - val_loss: 0.4429 - val_accuracy: 0.8714\n",
      "Epoch 19/100\n",
      "125/125 [==============================] - 24s 194ms/step - loss: 0.2824 - accuracy: 0.9146 - val_loss: 0.4388 - val_accuracy: 0.8733\n",
      "Epoch 20/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.2728 - accuracy: 0.9173 - val_loss: 0.4379 - val_accuracy: 0.8746\n",
      "Epoch 21/100\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 0.2631 - accuracy: 0.9204 - val_loss: 0.4407 - val_accuracy: 0.8757\n",
      "Epoch 22/100\n",
      "125/125 [==============================] - 24s 194ms/step - loss: 0.2539 - accuracy: 0.9232 - val_loss: 0.4438 - val_accuracy: 0.8747\n",
      "Epoch 23/100\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 0.2454 - accuracy: 0.9257 - val_loss: 0.4425 - val_accuracy: 0.8754\n",
      "Epoch 24/100\n",
      "125/125 [==============================] - 24s 194ms/step - loss: 0.2369 - accuracy: 0.9283 - val_loss: 0.4428 - val_accuracy: 0.8750\n",
      "Epoch 25/100\n",
      "125/125 [==============================] - 27s 214ms/step - loss: 0.2293 - accuracy: 0.9303 - val_loss: 0.4414 - val_accuracy: 0.8780\n",
      "Epoch 26/100\n",
      "125/125 [==============================] - 29s 233ms/step - loss: 0.2212 - accuracy: 0.9328 - val_loss: 0.4486 - val_accuracy: 0.8765\n",
      "Epoch 27/100\n",
      "125/125 [==============================] - 28s 224ms/step - loss: 0.2146 - accuracy: 0.9346 - val_loss: 0.4482 - val_accuracy: 0.8768\n",
      "Epoch 28/100\n",
      "125/125 [==============================] - 24s 189ms/step - loss: 0.2071 - accuracy: 0.9370 - val_loss: 0.4507 - val_accuracy: 0.8772\n",
      "Epoch 29/100\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 0.2009 - accuracy: 0.9387 - val_loss: 0.4520 - val_accuracy: 0.8767\n",
      "Epoch 30/100\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 0.1944 - accuracy: 0.9408 - val_loss: 0.4582 - val_accuracy: 0.8782\n",
      "Epoch 31/100\n",
      "125/125 [==============================] - 27s 212ms/step - loss: 0.1885 - accuracy: 0.9423 - val_loss: 0.4566 - val_accuracy: 0.8776\n",
      "Epoch 32/100\n",
      "125/125 [==============================] - 28s 227ms/step - loss: 0.1826 - accuracy: 0.9442 - val_loss: 0.4616 - val_accuracy: 0.8784\n",
      "Epoch 33/100\n",
      "125/125 [==============================] - 28s 219ms/step - loss: 0.1771 - accuracy: 0.9458 - val_loss: 0.4675 - val_accuracy: 0.8771\n",
      "Epoch 34/100\n",
      "125/125 [==============================] - 28s 224ms/step - loss: 0.1715 - accuracy: 0.9474 - val_loss: 0.4699 - val_accuracy: 0.8775\n",
      "Epoch 35/100\n",
      "125/125 [==============================] - 30s 242ms/step - loss: 0.1666 - accuracy: 0.9488 - val_loss: 0.4767 - val_accuracy: 0.8771\n",
      "Epoch 36/100\n",
      "125/125 [==============================] - 28s 226ms/step - loss: 0.1615 - accuracy: 0.9506 - val_loss: 0.4780 - val_accuracy: 0.8773\n",
      "Epoch 37/100\n",
      "125/125 [==============================] - 30s 241ms/step - loss: 0.1575 - accuracy: 0.9516 - val_loss: 0.4848 - val_accuracy: 0.8771\n",
      "Epoch 38/100\n",
      "125/125 [==============================] - 29s 231ms/step - loss: 0.1528 - accuracy: 0.9529 - val_loss: 0.4836 - val_accuracy: 0.8775\n",
      "Epoch 39/100\n",
      "125/125 [==============================] - 27s 215ms/step - loss: 0.1479 - accuracy: 0.9545 - val_loss: 0.4888 - val_accuracy: 0.8776\n",
      "Epoch 40/100\n",
      "125/125 [==============================] - 27s 219ms/step - loss: 0.1445 - accuracy: 0.9553 - val_loss: 0.4942 - val_accuracy: 0.8765\n",
      "Epoch 41/100\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 0.1403 - accuracy: 0.9567 - val_loss: 0.4998 - val_accuracy: 0.8773\n",
      "Epoch 42/100\n",
      "125/125 [==============================] - 27s 214ms/step - loss: 0.1368 - accuracy: 0.9576 - val_loss: 0.5017 - val_accuracy: 0.8780\n",
      "Epoch 43/100\n",
      "125/125 [==============================] - 28s 221ms/step - loss: 0.1330 - accuracy: 0.9587 - val_loss: 0.5095 - val_accuracy: 0.8768\n",
      "Epoch 44/100\n",
      "125/125 [==============================] - 26s 203ms/step - loss: 0.1296 - accuracy: 0.9594 - val_loss: 0.5097 - val_accuracy: 0.8773\n",
      "Epoch 45/100\n",
      "125/125 [==============================] - 24s 193ms/step - loss: 0.1264 - accuracy: 0.9608 - val_loss: 0.5156 - val_accuracy: 0.8766\n",
      "Epoch 46/100\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 0.1233 - accuracy: 0.9615 - val_loss: 0.5158 - val_accuracy: 0.8778\n",
      "Epoch 47/100\n",
      "125/125 [==============================] - 27s 216ms/step - loss: 0.1200 - accuracy: 0.9625 - val_loss: 0.5269 - val_accuracy: 0.8778\n",
      "Epoch 48/100\n",
      "125/125 [==============================] - 27s 214ms/step - loss: 0.1170 - accuracy: 0.9635 - val_loss: 0.5296 - val_accuracy: 0.8770\n",
      "Epoch 49/100\n",
      "125/125 [==============================] - 28s 221ms/step - loss: 0.1141 - accuracy: 0.9645 - val_loss: 0.5362 - val_accuracy: 0.8767\n",
      "Epoch 50/100\n",
      "125/125 [==============================] - 28s 225ms/step - loss: 0.1113 - accuracy: 0.9651 - val_loss: 0.5386 - val_accuracy: 0.8778\n",
      "Epoch 51/100\n",
      "125/125 [==============================] - 29s 227ms/step - loss: 0.1087 - accuracy: 0.9659 - val_loss: 0.5435 - val_accuracy: 0.8768\n",
      "Epoch 52/100\n",
      "125/125 [==============================] - 28s 222ms/step - loss: 0.1063 - accuracy: 0.9667 - val_loss: 0.5501 - val_accuracy: 0.8770\n",
      "Epoch 53/100\n",
      "125/125 [==============================] - 29s 229ms/step - loss: 0.1038 - accuracy: 0.9673 - val_loss: 0.5574 - val_accuracy: 0.8763\n",
      "Epoch 54/100\n",
      "125/125 [==============================] - 28s 226ms/step - loss: 0.1012 - accuracy: 0.9680 - val_loss: 0.5552 - val_accuracy: 0.8765\n",
      "Epoch 55/100\n",
      "125/125 [==============================] - 28s 227ms/step - loss: 0.0991 - accuracy: 0.9687 - val_loss: 0.5617 - val_accuracy: 0.8758\n",
      "Epoch 56/100\n",
      "125/125 [==============================] - 28s 221ms/step - loss: 0.0966 - accuracy: 0.9693 - val_loss: 0.5687 - val_accuracy: 0.8763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "125/125 [==============================] - 29s 234ms/step - loss: 0.0950 - accuracy: 0.9698 - val_loss: 0.5709 - val_accuracy: 0.8766\n",
      "Epoch 58/100\n",
      "125/125 [==============================] - 29s 231ms/step - loss: 0.0927 - accuracy: 0.9704 - val_loss: 0.5797 - val_accuracy: 0.8753\n",
      "Epoch 59/100\n",
      "125/125 [==============================] - 30s 240ms/step - loss: 0.0907 - accuracy: 0.9711 - val_loss: 0.5799 - val_accuracy: 0.8757\n",
      "Epoch 60/100\n",
      "125/125 [==============================] - 33s 267ms/step - loss: 0.0886 - accuracy: 0.9715 - val_loss: 0.5841 - val_accuracy: 0.8759\n",
      "Epoch 61/100\n",
      "125/125 [==============================] - 32s 255ms/step - loss: 0.0869 - accuracy: 0.9723 - val_loss: 0.5828 - val_accuracy: 0.8761\n",
      "Epoch 62/100\n",
      "125/125 [==============================] - 27s 215ms/step - loss: 0.0847 - accuracy: 0.9730 - val_loss: 0.5945 - val_accuracy: 0.8753\n",
      "Epoch 63/100\n",
      "125/125 [==============================] - 28s 224ms/step - loss: 0.0834 - accuracy: 0.9731 - val_loss: 0.5987 - val_accuracy: 0.8757\n",
      "Epoch 64/100\n",
      "125/125 [==============================] - 28s 227ms/step - loss: 0.0815 - accuracy: 0.9736 - val_loss: 0.6030 - val_accuracy: 0.8763\n",
      "Epoch 65/100\n",
      "125/125 [==============================] - 28s 225ms/step - loss: 0.0799 - accuracy: 0.9742 - val_loss: 0.6091 - val_accuracy: 0.8756\n",
      "Epoch 66/100\n",
      "125/125 [==============================] - 28s 223ms/step - loss: 0.0781 - accuracy: 0.9747 - val_loss: 0.6105 - val_accuracy: 0.8758\n",
      "Epoch 67/100\n",
      "125/125 [==============================] - 30s 236ms/step - loss: 0.0770 - accuracy: 0.9749 - val_loss: 0.6138 - val_accuracy: 0.8755\n",
      "Epoch 68/100\n",
      "125/125 [==============================] - 30s 240ms/step - loss: 0.0754 - accuracy: 0.9755 - val_loss: 0.6171 - val_accuracy: 0.8754\n",
      "Epoch 69/100\n",
      "125/125 [==============================] - 31s 246ms/step - loss: 0.0735 - accuracy: 0.9760 - val_loss: 0.6195 - val_accuracy: 0.8759\n",
      "Epoch 70/100\n",
      "125/125 [==============================] - 30s 236ms/step - loss: 0.0725 - accuracy: 0.9763 - val_loss: 0.6273 - val_accuracy: 0.8756\n",
      "Epoch 71/100\n",
      "125/125 [==============================] - 36s 291ms/step - loss: 0.0708 - accuracy: 0.9766 - val_loss: 0.6244 - val_accuracy: 0.8755\n",
      "Epoch 72/100\n",
      "125/125 [==============================] - 32s 253ms/step - loss: 0.0698 - accuracy: 0.9770 - val_loss: 0.6330 - val_accuracy: 0.8757\n",
      "Epoch 73/100\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 0.0682 - accuracy: 0.9776 - val_loss: 0.6411 - val_accuracy: 0.8752\n",
      "Epoch 74/100\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 0.0673 - accuracy: 0.9776 - val_loss: 0.6419 - val_accuracy: 0.8753\n",
      "Epoch 75/100\n",
      "125/125 [==============================] - 31s 251ms/step - loss: 0.0660 - accuracy: 0.9780 - val_loss: 0.6476 - val_accuracy: 0.8749\n",
      "Epoch 76/100\n",
      "125/125 [==============================] - 33s 261ms/step - loss: 0.0648 - accuracy: 0.9783 - val_loss: 0.6520 - val_accuracy: 0.8746\n",
      "Epoch 77/100\n",
      "125/125 [==============================] - 31s 246ms/step - loss: 0.0637 - accuracy: 0.9783 - val_loss: 0.6563 - val_accuracy: 0.8749\n",
      "Epoch 78/100\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.0628 - accuracy: 0.9789 - val_loss: 0.6596 - val_accuracy: 0.8748\n",
      "Epoch 79/100\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 0.0618 - accuracy: 0.9790 - val_loss: 0.6610 - val_accuracy: 0.8749\n",
      "Epoch 80/100\n",
      "125/125 [==============================] - 32s 257ms/step - loss: 0.0605 - accuracy: 0.9795 - val_loss: 0.6652 - val_accuracy: 0.8744\n",
      "Epoch 81/100\n",
      "125/125 [==============================] - 31s 248ms/step - loss: 0.0596 - accuracy: 0.9799 - val_loss: 0.6649 - val_accuracy: 0.8760\n",
      "Epoch 82/100\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 0.0588 - accuracy: 0.9799 - val_loss: 0.6746 - val_accuracy: 0.8743\n",
      "Epoch 83/100\n",
      "125/125 [==============================] - 25s 204ms/step - loss: 0.0576 - accuracy: 0.9803 - val_loss: 0.6767 - val_accuracy: 0.8747\n",
      "Epoch 84/100\n",
      "125/125 [==============================] - 26s 211ms/step - loss: 0.0568 - accuracy: 0.9805 - val_loss: 0.6874 - val_accuracy: 0.8733\n",
      "Epoch 85/100\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 0.0560 - accuracy: 0.9805 - val_loss: 0.6853 - val_accuracy: 0.8745\n",
      "Epoch 86/100\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 0.0550 - accuracy: 0.9811 - val_loss: 0.6819 - val_accuracy: 0.8742\n",
      "Epoch 87/100\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 0.0541 - accuracy: 0.9813 - val_loss: 0.6867 - val_accuracy: 0.8745\n",
      "Epoch 88/100\n",
      "125/125 [==============================] - 26s 204ms/step - loss: 0.0531 - accuracy: 0.9815 - val_loss: 0.6974 - val_accuracy: 0.8737\n",
      "Epoch 89/100\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.0525 - accuracy: 0.9817 - val_loss: 0.7013 - val_accuracy: 0.8734\n",
      "Epoch 90/100\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 0.0518 - accuracy: 0.9819 - val_loss: 0.6979 - val_accuracy: 0.8738\n",
      "Epoch 91/100\n",
      "125/125 [==============================] - 27s 215ms/step - loss: 0.0510 - accuracy: 0.9821 - val_loss: 0.7028 - val_accuracy: 0.8749\n",
      "Epoch 92/100\n",
      "125/125 [==============================] - 26s 205ms/step - loss: 0.0507 - accuracy: 0.9821 - val_loss: 0.7070 - val_accuracy: 0.8735\n",
      "Epoch 93/100\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.0497 - accuracy: 0.9825 - val_loss: 0.7062 - val_accuracy: 0.8751\n",
      "Epoch 94/100\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 0.0489 - accuracy: 0.9827 - val_loss: 0.7125 - val_accuracy: 0.8743\n",
      "Epoch 95/100\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 0.0482 - accuracy: 0.9830 - val_loss: 0.7154 - val_accuracy: 0.8728\n",
      "Epoch 96/100\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 0.0476 - accuracy: 0.9831 - val_loss: 0.7193 - val_accuracy: 0.8728\n",
      "Epoch 97/100\n",
      "125/125 [==============================] - 26s 204ms/step - loss: 0.0472 - accuracy: 0.9833 - val_loss: 0.7204 - val_accuracy: 0.8737\n",
      "Epoch 98/100\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 0.0466 - accuracy: 0.9834 - val_loss: 0.7232 - val_accuracy: 0.8731\n",
      "Epoch 99/100\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 0.0459 - accuracy: 0.9835 - val_loss: 0.7253 - val_accuracy: 0.8734\n",
      "Epoch 100/100\n",
      "125/125 [==============================] - 25s 204ms/step - loss: 0.0455 - accuracy: 0.9837 - val_loss: 0.7271 - val_accuracy: 0.8736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s\\assets\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Now we will fit our model.\n",
    "Before fitting our model, we choosed softmax as activation function for output layer and rmsprop as optimizer .\n",
    "'''\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "# Save model\n",
    "model.save(\"s2s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa6e1d2",
   "metadata": {},
   "source": [
    "## Run inference (sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e057719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling models\n",
    "# Restore the model and construct the encoder and decoder.\n",
    "model = keras.models.load_model(\"s2s\")\n",
    "#encode input and retrieve initial decoder state\n",
    "\n",
    "''' Encode input and retrieve initial decoder state '''\n",
    "encoder_inputs = model.input[0]  # input_1\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "'''We will run one step of decoder with this initial state and a \"start of sequence\" token as target. \n",
    "Output will be the next target token.'''\n",
    "decoder_inputs = model.input[1]  # input_2\n",
    "decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = model.layers[3]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\n",
    ")\n",
    "'''we will repeat this step with the current target token and current states'''\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60de9fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 563ms/step\n",
      "1/1 [==============================] - 1s 550ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: En route !\n",
      "\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: En route !\n",
      "\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: En route !\n",
      "\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: En route !\n",
      "\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salut.\n",
      "\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salut.\n",
      "\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Filez !\n",
      "\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Filez !\n",
      "\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Filez !\n",
      "\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Filez !\n",
      "\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Filez !\n",
      "\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Filez !\n",
      "\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Filez !\n",
      "\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Filez !\n",
      "\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Filez !\n",
      "\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Filez !\n",
      "\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Filez !\n",
      "\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Filez !\n",
      "\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Filez !\n",
      "\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Filez !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''In this step we will try to generate some decoded sentences'''\n",
    "for seq_index in range(20):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(\"-\")\n",
    "    print(\"Input sentence:\", input_texts[seq_index])\n",
    "    print(\"Decoded sentence:\", decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9a435a",
   "metadata": {},
   "source": [
    "In this exemple we build a simple character-level recurrent sequence-to-sequence model. We tried to translate short English senctences into French, sequence-to-sequence, that mean character by charater. With large text that is unusual because it can provide poor style of expression. The main idea is that an encoder LSTM turns input sequences to 2 state vectors and another decoder LSTM is trained to turn the target sequences into the same sequence but offset by one timestep in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
